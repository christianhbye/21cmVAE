# -*- coding: utf-8 -*-
"""21cm_em_12_sim_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qSc9eObPxyg0Srh4lBnIH2SR5MdUhfAt
"""

import time

HOME_DIR = '/content' # replace this with wherever you want to run the code
DATA_DIR = 'data' # subdir of HOME_DIR where data is stored
WORK_DIR = '21cm_emulator_GAN_%d'%(round(time.time())) # this is the working directory, everything gets saved here (except data)

"""**DATA**"""

import os
import numpy as np
import tensorflow as tf
import h5py
import matplotlib.pyplot as plt

os.chdir(HOME_DIR)
if not DATA_DIR in os.listdir():
  os.mkdir(DATA_DIR)
os.chdir(DATA_DIR)

## load the datasets

with h5py.File('dataset.h5', 'r') as hf:
  signal_train = hf['signal_train'][:]
  signal_val = hf['signal_val'][:]
  signal_test = hf['signal_test'][:]
  par_train = hf['par_train'][:]
  par_val = hf['par_val'][:]
  par_test = hf['par_test'][:]

# the astrophysical parameters
par_labels = ['fstar', 'Vc', 'fx', 'tau', 'alpha', 'nu_min', 'Rmfp']

## get the amplitudes of the signals

train_amplitudes = np.max(np.abs(signal_train), axis=-1)
val_amplitudes = np.max(np.abs(signal_val), axis=-1)
test_amplitudes = np.max(np.abs(signal_test), axis=-1)

z = np.arange(5, 50+0.1, 0.1) # redshifts

def xticks_calculator(redshift):
  rest_frequency = 1420405751.7667 # rest frequency in Hz
  freqs = rest_frequency/(1+redshift)
  freqs /= 1e6 # convert to MHz
  return freqs

nu = xticks_calculator(z) # frequencies

## preprocessing of signals

def preproc(signal):
  proc_signal = signal.copy()
  proc_signal -= np.mean(signal_train, axis=0) # subtract mean
  proc_signal /= np.std(signal_train) # divide by standard deviation
  return proc_signal

def unpreproc(signal):
  proc_signal = signal.copy()
  proc_signal *= np.std(signal_train)
  proc_signal += np.mean(signal_train, axis=0)
  return proc_signal

# preprocessed signals
signal_train_preproc = preproc(signal_train)
signal_val_preproc = preproc(signal_val)
signal_test_preproc = preproc(signal_test)

"""**Creating the models**"""

from tensorflow.keras.layers import Input, Dense, Lambda
from tensorflow.keras.models import Model

from tensorflow.keras import backend as K

# To create the VAE latent layer
def sampling(args):
    """Reparameterization trick by sampling from an isotropic unit Gaussian.

    # Arguments
        args (tensor): mean and log of variance of Q(z|X)

    # Returns
        z (tensor): sampled latent vector
    """

    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    # by default, random_normal has mean = 0 and std = 1.0
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon

## the emulator loss function:
## the square of the FoM in the paper, in units of standard deviation
## since the signals are preproccesed

from tensorflow.keras.losses import mse

def em_loss_fcn(y_true, y_pred):
  # mse loss
  signal = y_true[:, 0:-1]
  amplitude = y_true[:, -1]/tf.math.reduce_std(signal_train)
  loss = mse(y_pred, signal)
  loss /= K.square(amplitude)
  return loss

from tensorflow.keras import optimizers


hp_lambda = K.variable(1) # for KL annealing

# fixed hyperparameters 
activation_func = 'relu' # activation function
vae_lr = 0.01 # learning rate of VAE
em_lr = 0.01 # learning rate of emulator
# initialize the Adam optimizers
vae_opt = optimizers.Adam(learning_rate=vae_lr)
em_opt = optimizers.Adam(learning_rate=em_lr)

# Parameters that control the learning rate schedule:
vae_lr_factor = 0.7 # factor LR is multiplied with when reduced
vae_lr_patience = 5 # number of epochs to wait before reducing LR
vae_min_lr = 1e-6 # minimum allowed LR
em_lr_factor = 0.7
em_lr_patience = 5
em_min_lr = 1e-6

# if the loss doesn't decrease to less than this factor, LR is reduced:
lr_max_factor = 0.95

# for early stopping
early_stop_patience = 15 # number of epochs to wait before stopping training

# if the loss doesn't decrease to less than this factor, training is stopped:
es_max_factor = 0.99

def build_models(hps, layer_hps, vae_optimizer=vae_opt, em_optimizer=em_opt):
  '''
  Function that build the two neural networks.
  
  Input parameters (see below for examples):
  hps: hyperparameters, a dictionary with values for the latent layer 
  dimensionality, beta, and gamma
  layer_hps: the hyperparameters controlling the number of layers and 
  their dimensionalities, packed into nested lists
  vae_optimizer: a keras optimizer instance for the VAE, can be left untouched
  em_optimizer: same as above, but for the emulator

  Returns the VAE and the emulator as keras model objects
  '''
  encoding_hidden_dims = layer_hps[0] # the layers of the encoder
  vae_input = Input(shape=(signal_train.shape[1],)) # input layer for VAE
  # loop over the number of layers and build the encoder part of the VAE
  # with layers of the given dimensionalities
  for i, dim in enumerate(encoding_hidden_dims):
    if i == 0:
      input_layer = vae_input # the first layer takes input from the input layer
    else:
      input_layer = x # subsequent layers take input from the previous layer
    # using dense (fuly connected) layers
    x = Dense(dim, activation=activation_func, name='encoder_hidden_layer_'+str(i))(input_layer)
  latent_dim = hps['latent_dim'] # dimensionality of latent layer
  z_mean = Dense(latent_dim, name='z_mean')(x) # mean values for the Gaussians
  # in the latent layer
  z_log_var = Dense(latent_dim, name='z_log_var')(x) # log variance of Gaussians
  # sample points from the Gaussians with given mean and variance 
  z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

  # now, the same procedure for the decoder as for the encoder
  decoding_hidden_dims = layer_hps[1] 
  decoder_layers = [] # list of decoder layers
  for i, dim in enumerate(decoding_hidden_dims):
    if i == 0:
      input_layer = z
    else:
      input_layer = x
    decoder_layer = Dense(dim, activation=activation_func, name='decoder_hidden_layer_'+str(i))
    decoder_layers.append(decoder_layer) # add to decoder layer list
    x = decoder_layer(input_layer)
  
  decoder_output_layer = Dense(signal_train.shape[1]) # output of decoder
  decoder_layers.append(decoder_output_layer)
  decoded = decoder_output_layer(x)
  vae_output = decoded

  # create the VAE
  vae = Model(vae_input, vae_output, name='VAE')

  # the VAE loss function
  reconstruction_loss = mse(vae_input, vae_output) # mean squared error
  # get the unpreprocessed signal to get the amplitude
  unproc_signal = vae_input + K.constant(np.mean(signal_train, axis=0)/np.std(signal_train))
  amplitude = K.max(K.abs(unproc_signal), axis=-1)
  reconstruction_loss /= K.square(amplitude)
  # KL-part of the loss:
  kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
  kl_loss = K.sum(kl_loss, axis=-1)
  kl_loss *= -0.5
  orig_dim = signal_train.shape[-1]
  # get hyperparameters:
  gamma = hps['gamma']
  beta = hps['beta']
  vae_loss_fcn = (orig_dim*reconstruction_loss + beta*hp_lambda*kl_loss)/(orig_dim * gamma)
  vae.add_loss(vae_loss_fcn) # add the loss function to the model
  vae.compile(optimizer=vae_optimizer) # compile the model with the optimizer
  
  # make the emulator in the same way as the encoder
  em_input_par = Input(shape=(X_train.shape[1],), name='em_input')
  em_hidden_dims = layer_hps[2]
  for i, dim in enumerate(em_hidden_dims):
    if i == 0:
      input_layer = em_input_par
    else:
      input_layer = x
    x = Dense(dim, activation=activation_func, name='em_hidden_layer_'+str(i))(input_layer)
  
  # the latent layer of the emulator
  autoencoder_par = Dense(latent_dim, name='em_autoencoder')(x)

  # the decoder layers are shared so we just get them from the list made above
  for i, decoder_layer in enumerate(decoder_layers):
    if i == 0:
      output_par_in = autoencoder_par
    else:
      output_par_in = output_par
    output_par = decoder_layer(output_par_in)
  
  # compile the emulator
  em_output = output_par
  emulator = Model(em_input_par, em_output, name='emulator')
  em_opt = optimizers.Adam(learning_rate=em_lr)
  emulator.compile(optimizer=em_optimizer, loss=em_loss_fcn)

  return vae, emulator

"""**Training**"""

from tensorflow.keras import callbacks

# KL annealing
def anneal_schedule(epoch):
  '''
  Annealing schedule, linear increase from 0 to 1 over 10 epochs
  '''
  return min(epoch * 0.1, 1.)

class AneelingCallback(callbacks.Callback):
  def __init__(self, schedule, variable):
    super(AneelingCallback, self).__init__()
    self.schedule = schedule
    self.variable = variable
        
  def on_epoch_begin(self, epoch, logs={}):
    value = self.schedule(epoch)
    assert type(value) == float, ('The output of the "schedule" function should be float.')
    K.set_value(self.variable, value)

kl_annealing = AneelingCallback(anneal_schedule, hp_lambda)

def plateau_check(model, patience, max_factor, vae_loss_val, em_loss_val):
  '''
  Helper function for reduce_lr(). Checks if the validation loss has stopped
  decreasing as defined by the parameters.
  model = "vae" or "emulator"
  Returns True (reduce LR) or False (don't reduce LR)
  '''
  
  if model == "vae":
    loss_list = vae_loss_val
  elif model == "emulator":
    loss_list = em_loss_val
  else:
    print('Invalid input parameter "model". Must be "vae" or "emulator".')

  if not len(loss_list) > patience: # there is not enough training to compare
    return False

  max_loss = max_factor*loss_list[-(1+patience)] # max acceptable loss

  count = 0
  while count < patience:
    if loss_list[-(1+count)] > max_loss:
      count += 1
      continue
    else:
      break
  if count == patience: # the last [patience] losses are all too large: reduce lr
    return True
  else:
    return False

def reduce_lr(model, factor, min_lr):
  '''
  Manual implementation of https://keras.io/api/callbacks/reduce_lr_on_plateau/. 
  model: "vae" or "em"
  '''
  assert min_lr >= 0, "min_lr must be non-negative"
  old_lr = K.get_value(model.optimizer.learning_rate) # get current LR
  if old_lr * factor <= min_lr and old_lr > min_lr:
    new_lr = min_lr
    print('Reached min_lr, lr will not continue to decrease! {}_lr = {:.3e}'.format(model.name, new_lr))
  elif old_lr == min_lr:
    pass
  else:
    new_lr = old_lr * factor
    print('Reduce learning rate, {}_lr = {:.3e}'.format(model.name, new_lr))
    K.set_value(model.optimizer.learning_rate, new_lr)

def early_stop(patience, max_factor, vae_loss_val, em_loss_val):
  '''
  Manual implementation of https://keras.io/api/callbacks/early_stopping/.
  '''
  if not len(vae_loss_val) > patience: # there is not enough training to compare
    return True

  vae_max_loss = vae_loss_val[-(1+patience)] * max_factor # the max acceptable loss
  em_max_loss = em_loss_val[-(1+patience)] * max_factor # the max acceptable loss

  count = 0
  while count < patience:
    if vae_loss_val[-(1+count)] > vae_max_loss and em_loss_val[-(1+count)] > em_max_loss:
      count += 1
      continue
    else:
      break
  if count == patience: # the last [patience] losses are all too large: stop training
    print("Early stopping!")
    return False # keep_going = False, i.e. stop early
  else:
    return True # keep_going = True, continue

## create minibatches

batch_size = 256
 
def create_batch(x_train, y_train, amplitudes):
  '''
  amplitudes = amplitdue of training signals / np.std(signal_train)
  '''

  dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, amplitudes)).shuffle(1000) 
  # Combines consecutive elements of this dataset into batches. 
  dataset = dataset.batch(batch_size, drop_remainder = True).prefetch(1) 
  # Creates a Dataset that prefetches elements from this dataset 
  return dataset

def par_transform(params_train, params_val, params_test):
  '''
  Take log of three first columns (fx, Vc, fstar), then map the parameters to 
  [-1, 1]. Returns new parameters.'''
  # training set
  cols12_tr = params_train[:, :2].copy() # fstar and Vc
  fx_tr = params_train[:, 2].copy() # fx
  fx_tr[fx_tr == 0] = 10**(-6) # to avoid -inf in cases where fx == 0
  newcols12_tr = np.log(cols12_tr) # take log, should strictly speaking be
  # log10, but the difference is just a scale factor
  newfx_tr = np.log(fx_tr) # log of fx

  # validation set
  cols12_val = params_val[:, :2].copy()
  fx_val = params_val[:, 2].copy()
  fx_val[fx_val == 0] = 10**(-6) # to avoid -inf in cases where fx == 0
  newcols12_val = np.log(cols12_val)
  newfx_val = np.log(fx_val)

  # test set
  cols12_test = params_test[:, :2].copy()
  fx_test = params_test[:, 2].copy()
  fx_test[fx_test == 0] = 10**(-6) # to avoid -inf in cases where fx == 0
  newcols12_test = np.log(cols12_test)
  newfx_test = np.log(fx_test)

  # initialize arrays with processed parameters, first copy old parameter:

  newparams_tr = np.empty(params_train.shape)
  newparams_tr[:, :2] = newcols12_tr
  newparams_tr[:, 2] = newfx_tr
  newparams_tr[:, 3:] = params_train[:, 3:].copy()

  newparams_val = np.empty(params_val.shape)
  newparams_val[:, :2] = newcols12_val
  newparams_val[:, 2] = newfx_val
  newparams_val[:, 3:] = params_val[:, 3:].copy()

  newparams_test = np.empty(params_test.shape)
  newparams_test[:, :2] = newcols12_test
  newparams_test[:, 2] = newfx_test
  newparams_test[:, 3:] = params_test[:, 3:].copy()

  # get the max and min values of each parameter in the training set
  maximum = np.max(newparams_tr, axis=0)
  minimum = np.min(newparams_tr, axis=0)

  # subtract min, divide by (max-min), multiply by 2 and subtract 1 to get
  # parameters in the range [-1, 1]
  newparams_tr -= minimum
  newparams_tr /= (maximum - minimum)
  newparams_tr = 2*newparams_tr - 1
  newparams_val -= minimum
  newparams_val /= (maximum - minimum)
  newparams_val = 2*newparams_val - 1
  newparams_test -= minimum
  newparams_test /= (maximum - minimum)
  newparams_test = 2*newparams_test - 1

  return newparams_tr, newparams_val, newparams_test

# Input variables
X_train, X_val, X_test = par_transform(par_train, par_val, par_test)

# Output variables
y_train = signal_train_preproc
y_val = signal_val_preproc

# create the training and validation minibatches
dataset = create_batch(X_train, y_train, train_amplitudes)
val_dataset = create_batch(X_val, y_val, val_amplitudes)

def train_models(vae, emulator, dataset, val_dataset, epochs = 350):
  '''
  Function that train the models simultaneously

  Input parameters:
  vae: Keras model object, the VAE
  emulator: Keras model object, the emulator
  dataset: batches from training dataset
  val_dataset: batches from validation dataset
  epochs: max number of epochs to train for, early stopping may stop it before
  '''
  # initialize lists of training losses and validation losses
  vae_loss = []
  vae_loss_val = []
  em_loss = []
  em_loss_val = []
  
  # Did the model loss plateau?
  plateau_vae = False
  plateau_em = False
  vae_reduced_lr = 0 # epochs since last time lr was reduced
  em_reduced_lr = 0 # epochs since last time lr was reduced

  @tf.function
  def run_train_step(batch):
    '''
    Function that trains the VAE and emulator for one batch. Returns the losses
    for that specific batch.
    '''
    params = batch[0]
    signal = batch[1]
    amp_raw = batch[2] # amplitudes, raw because we need to reshape
    amplitudes = tf.expand_dims(amp_raw, axis=1) # reshape amplitudes
    signal_amplitudes = tf.concat((signal, amplitudes), axis=1) # both signal and amplitude
    with tf.GradientTape() as tape:
      vae_pred = vae(signal) # apply vae to input signal
      vae_batch_loss = vae.losses # get the loss
    # back-propagate losses for the VAE
    vae_gradients = tape.gradient(vae_batch_loss, vae.trainable_weights)
    vae.optimizer.apply_gradients(zip(vae_gradients, vae.trainable_weights))
    # same procedure for emulator
    with tf.GradientTape() as tape:
      em_pred = emulator(params)
      em_batch_loss = em_loss_fcn(signal_amplitudes, em_pred)
    em_gradients = tape.gradient(em_batch_loss, emulator.trainable_weights)
    emulator.optimizer.apply_gradients(zip(em_gradients, emulator.trainable_weights))
    return vae_batch_loss, em_batch_loss

  # the training loop
  for i in range(epochs):
    epoch = int(i+1)
    print("\nEpoch {}/{}".format(epoch, epochs))

    # reduce lr if necessary
    if plateau_vae and vae_reduced_lr >= 5:
      reduce_lr(vae, vae_lr_factor, vae_min_lr)
      vae_reduced_lr = 0
    if plateau_em and em_reduced_lr >=5 :
      reduce_lr(emulator, em_lr_factor, em_min_lr)
      em_reduced_lr = 0

    vae_batch_losses = []
    val_vae_batch_losses = []
    em_batch_losses = []
    val_em_batch_losses = []
      
    # KL annealing, updates hp_lambda
    kl_annealing.on_epoch_begin(epoch)

    # loop through the batches and train the models on each batch
    for batch in dataset:
      vae_batch_loss, em_batch_loss = run_train_step(batch)
      vae_batch_losses.append(vae_batch_loss) # append VAE train loss for this batch
      em_batch_losses.append(em_batch_loss) # append emulator train loss for this batch

    # loop through the validation batches, we are not training on them but
    # just evaluating and tracking the performance
    for batch in val_dataset:
      param_val = batch[0]
      signal_val = batch[1]
      amp_val = tf.expand_dims(batch[2], axis=1)
      val_signal_amplitudes = tf.concat((signal_val, amp_val), axis=1)        
      val_em_batch_loss = emulator.test_on_batch(param_val, val_signal_amplitudes)
      val_vae_batch_loss = vae.test_on_batch(signal_val, signal_val)
      val_vae_batch_losses.append(val_vae_batch_loss)
      val_em_batch_losses.append(val_em_batch_loss)

    vae_loss_epoch = K.mean(tf.convert_to_tensor(vae_batch_losses)) # average VAE train loss over this epoch
    em_loss_epoch = K.mean(tf.convert_to_tensor(em_batch_losses)) # average emulator train loss
    print('VAE train loss: {:.4f}'.format(vae_loss_epoch))
    print('Emulator train loss: {:.4f}'.format(em_loss_epoch))

    # in case a loss is NaN
    # this is unusal, but not a big deal, just restart the training 
    # (otherwise the loss just stays NaN)
    if np.isnan(vae_loss_epoch) or np.isnan(em_loss_epoch):
      print("Loss is NaN, restart training") 
      break
      
    # save each epoch loss to a list with all epochs
    vae_loss.append(vae_loss_epoch) 
    em_loss.append(em_loss_epoch)

    vae_loss_epoch_val = np.mean(val_vae_batch_losses) # average VAE train loss over this epoch
    em_loss_epoch_val = np.mean(val_em_batch_losses) # average emulator train loss
    vae_loss_val.append(vae_loss_epoch_val)
    em_loss_val.append(em_loss_epoch_val)      
    print('VAE val loss: {:.4f}'.format(vae_loss_epoch_val))
    print('Emulator val loss: {:.4f}'.format(em_loss_epoch_val))

    # save weights
    if epoch == 1: # save first epoch
      vae.save('checkpoints/best_vae')
      emulator.save('checkpoints/best_em')
    elif em_loss_val[-1] < np.min(em_loss_val[:-1]): # performance is better than prev epoch
      vae.save('checkpoints/best_vae')
      emulator.save('checkpoints/best_em')

    # early stopping?
    keep_going = early_stop(early_stop_patience, es_max_factor, vae_loss_val, em_loss_val)
    if not keep_going:
      break

    # check if loss stopped decreasing
    plateau_vae = plateau_check("vae", vae_lr_patience, lr_max_factor, vae_loss_val, em_loss_val)
    plateau_em = plateau_check("emulator", em_lr_patience, lr_max_factor, vae_loss_val, em_loss_val)

    vae_reduced_lr += 1
    em_reduced_lr += 1

  return vae_loss, vae_loss_val, em_loss, em_loss_val

os.chdir(HOME_DIR)
if not WORK_DIR in os.listdir():
  os.mkdir(WORK_DIR)
os.chdir(WORK_DIR)

def save_results(hps, layer_hps, vae, emulator, losses):
  '''
  Function that saves the results with hyperparameters
  '''
  fname = 'results'
  with open(fname+'.txt', 'w') as f:
    f.write('Hyperparameters:\n')
    f.write(str(hps))
    f.write('\n-----------\n')
    f.write('VAE Losses:\n')
    f.write('Train = {:.4f}'.format(np.min(losses[0])))
    f.write('\nTest = {:.4f}'.format(np.min(losses[1])))
    f.write('\n\nEmulator Losses:')
    f.write('\nTrain = {:.4f}'.format(np.min(losses[2])))
    f.write('\nTest = {:.4f}'.format(np.min(losses[3])))
    f.write('\n')
    vae.summary(print_fn=lambda x: f.write(x + '\n'))
    emulator.summary(print_fn=lambda x: f.write(x + '\n'))

## define the hyperparameters and start the training

hps = {}
hps['latent_dim'] = 22
hps['beta'] = 0.4
hps['gamma'] = 1.5

enc_dims = [96, 224, 288, 32]
dec_dims = [288]
em_dims = [160, 224]
layer_hps = [enc_dims, dec_dims, em_dims]

vae, emulator = build_models(hps, layer_hps)
losses = train_models(vae, emulator, dataset, val_dataset)

save_results(hps, layer_hps, vae, emulator, losses)

if not 'plots' in os.listdir():
  os.makedirs('plots')

# plot the training curves

vae_loss = losses[0]
vae_loss_val = losses[1]
em_loss = losses[2] 
em_loss_val = losses[3]

x_vals = np.arange(len(vae_loss)) + 1

plt.plot(x_vals, vae_loss, label='Train')
plt.plot(x_vals, vae_loss_val, label='Validation')
plt.ylabel('Loss')
plt.title('VAE')
plt.xlabel('Epoch')
plt.legend()
#plt.savefig('plots/tuned_vae_hist.png')
plt.show()

plt.plot(x_vals, np.log10(em_loss), label='Train')
plt.plot(x_vals, np.log10(em_loss_val), label='Validation')
plt.axvline(np.argmin(em_loss_val)+1, ls='--', c='black')
plt.ylabel('Loss')
plt.title('Emulator')
plt.xlabel('Epoch')
plt.legend()
#plt.savefig('plots/tuned_em_hist.png')
plt.show()

# get errors

vae = tf.keras.models.load_model('checkpoints/best_vae')
em = tf.keras.models.load_model('checkpoints/best_em', custom_objects={'em_loss_fcn': em_loss_fcn})

vae.save('tuned_vae.h5')
em.save('tuned_em.h5')
print("The lowest emulator validation loss corresponds to epoch {}".format(np.argmin(em_loss_val)+1))

vae_recon_test = unpreproc(vae.predict(signal_test_preproc))
vae_recon_val = unpreproc(vae.predict(signal_val_preproc))
vae_recon_train = unpreproc(vae.predict(signal_train_preproc))

train_predicted_signal = unpreproc(em.predict(X_train))
val_predicted_signal = unpreproc(em.predict(X_val))
test_predicted_signal = unpreproc(em.predict(X_test))

"""The metric used in the paper is the RMSE, normalized by the maximum amplitude of the true signal:

$$Error = \frac{\sqrt{mean[(T_{sim}(\nu)-T_{pred}(\nu))^2]}}{max|T_{sim}(\nu)|}$$

"""

def rms_error(signal, true_signal):
  num = np.sqrt(np.mean((signal - true_signal)**2, axis=1))
  den = np.max(np.abs(true_signal), axis=1)
  #den = 1 # to get the error in mK instead of relative error
  return num/den

vae_error_tr = rms_error(vae_recon_train, signal_train)
vae_error_val = rms_error(vae_recon_val, signal_val)
vae_error_test = rms_error(vae_recon_test, signal_test)

error_tr = rms_error(train_predicted_signal, signal_train)
error_val = rms_error(val_predicted_signal, signal_val)
error_test = rms_error(test_predicted_signal, signal_test)

plt.hist(vae_error_tr, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(vae_error_tr), label='Median', c='green', ls='--')
plt.axvline(np.mean(vae_error_tr), label='Mean', c='black', ls='--')
plt.legend()
plt.title('VAE, Train')
#plt.savefig('plots/vae_train_error.png')
plt.show()

plt.hist(vae_error_val, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(vae_error_val), label='Median', c='green', ls='--')
plt.axvline(np.mean(vae_error_val), label='Mean', c='black', ls='--')
plt.legend()
plt.title('VAE, Validation')
#plt.savefig('plots/vae_val_error.png')
plt.show()

plt.hist(vae_error_test, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(vae_error_test), label='Median', c='green', ls='--')
plt.axvline(np.mean(vae_error_test), label='Mean', c='black', ls='--')
plt.title('VAE, Test')
plt.legend()
#plt.savefig('plots/vae_test_error.png')
plt.show()

plt.hist(error_tr, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(error_tr), label='Median', c='green', ls='--')
plt.axvline(np.mean(error_tr), label='Mean', c='black', ls='--')
plt.legend()
plt.title('Train')
#plt.savefig('plots/em_train_error.png')
plt.show()

plt.hist(error_val, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(error_val), label='Median', c='green', ls='--')
plt.axvline(np.mean(error_val), label='Mean', c='black', ls='--')
plt.legend()
plt.title('Validation')
#plt.savefig('plots/em_val_error.png')
plt.show()

plt.hist(error_test, bins=100)
plt.axvline(0.0130, label='Median of 21cmGEM', c='orange', ls='--')
plt.axvline(0.0159, label='Mean of 21cmGEM', c='red', ls='--')
plt.axvline(np.median(error_test), label='Median', c='green', ls='--')
plt.axvline(np.mean(error_test), label='Mean', c='black', ls='--')
plt.title('Test')
plt.legend()
#plt.savefig('plots/em_test_error.png')
plt.show()

"""Mean and median errors across simulations are:"""

print('\nMean test error: {:.4f}'.format(np.mean(error_test)))
print('\nMedian test error: {:.4f}'.format(np.median(error_test)))
print('\nMean test error/21cmGEM mean test error: {:.4f}'.format(np.mean(error_test)/0.0159))
print('\nMedian test error/21cmGEM median test error: {:.4f}'.format(np.median(error_test)/0.0130))

# sample of signals

for i in range(10):
  plt.plot(test_predicted_signal[i])
plt.title("Signals predicted by emulator")
plt.savefig('plots/em_predicted_signals.png')
plt.show()


for i in range(10):
  plt.plot(vae_recon_test[i])
plt.title("Signals predicted by VAE")
plt.savefig('plots/vae_predicted_signals.png')
plt.show()

for i in range(10):
  plt.plot(vae_recon_test[i], ls='--', label='VAE')
  plt.plot(test_predicted_signal[i], ls='--', label='Emulator')
  plt.plot(signal_test[i], label='True')
  plt.legend()
  plt.savefig('plots/vae_vs_em_vs_true_'+str(i)+'.png')
  plt.show()
  plt.plot(vae_recon_test[i] - signal_test[i], label='VAE')
  plt.plot(test_predicted_signal[i] - signal_test[i], label='Emulator')
  plt.axhline(0, c='k', ls='--')
  plt.legend()
  plt.savefig('plots/vae_em_true_residuals_'+str(i)+'.png')
  plt.show()

# full_residuals = signal_test - test_predicted_signal
# max_residuals = np.argsort(np.max(np.abs(full_residuals), axis=1))[::-1]

# for i in range(10):
#   fig, axs = plt.subplots(2, 1, figsize=(7, 5), sharex=True, sharey=False, gridspec_kw={'height_ratios': [5, 3]})
#   fig.subplots_adjust(hspace=0)
#   axs[0].plot(z, signal_test[max_residuals[i]], label='True signal')
#   axs[0].plot(z, test_predicted_signal[max_residuals[i]], label='Predicted signal')
#   axs[0].legend()
#   axs[1].plot(z, signal_test[max_residuals[i]] - test_predicted_signal[max_residuals[i]])
#   plt.show()

with open('tuned_results.txt', 'w') as f:
  f.write('\nTraining set\n')
  f.write('Mean\n')
  f.write('VAE: {:.4f}'.format(np.mean(vae_error_tr)))
  f.write('\nEmulator: {:.4f}'.format(np.mean(error_tr)))
  f.write('\n------------\n')
  f.write('Median\n')
  f.write('VAE: {:.4f}'.format(np.median(vae_error_tr)))
  f.write('\nEmulator: {:.4f}'.format(np.median(error_tr)))
  f.write('\n------------\n------------\n')
  f.write('\nValidation set\n')
  f.write('Mean\n')
  f.write('VAE: {:.4f}'.format(np.mean(vae_error_val)))
  f.write('\nEmulator: {:.4f}'.format(np.mean(error_val)))
  f.write('\n------------\n')
  f.write('Median\n')
  f.write('VAE: {:.4f}'.format(np.median(vae_error_val)))
  f.write('\nEmulator: {:.4f}'.format(np.median(error_val)))
  f.write('\n------------\n------------\n')
  f.write('Test set\n')
  f.write('Mean\n')
  f.write('VAE: {:.4f}'.format(np.mean(vae_error_test)))
  f.write('\nEmulator: {:.4f}'.format(np.mean(error_test)))
  f.write('\n21cmGEM: 0.0159')
  f.write('\n------------\n')
  f.write('Median\n')
  f.write('VAE {:.4f}'.format(np.median(vae_error_test)))
  f.write('\nEmulator {:.4f}'.format(np.median(error_test)))
  f.write('\n21cmGEM: 0.0130')
  f.write('\n-----\n')
  f.write('\nMean test error/21cmGEM mean test error: {:.4f}'.format(np.mean(error_test)/0.0159))
  f.write('\nMedian test error/21cmGEM median test error: {:.4f}'.format(np.median(error_test)/0.0130))

em.save('emulator.h5')
vae.save('vae.h5')

os.listdir()

from google.colab import files
!zip -r wd .

files.download('wd.zip')

# from google.colab import files
# os.chdir(HOME_DIR)
# !rm -r sample_data
# !rm -r data
# !zip -r /content.zip /content/
# os.chdir('..')
# files.download('content.zip')
# os.chdir('content')
# ! rm -rf 21*